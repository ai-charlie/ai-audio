{"cells":[{"cell_type":"markdown","metadata":{},"source":["# DataLoader"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration default-b28597fbba130594\n","Reusing dataset csv (/home/zlq/.cache/huggingface/datasets/csv/default-b28597fbba130594/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aca522fd7f084454a4efa5685b1f8dde","version_major":2,"version_minor":0},"text/plain":["Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e17412ad8471497c9b65e81dfc6b7789","version_major":2,"version_minor":0},"text/plain":["Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['LABEL_0', 'LABEL_1', 'audio', 'emotion'],\n","        num_rows: 3350\n","    })\n","    test: Dataset({\n","        features: ['LABEL_0', 'LABEL_1', 'audio', 'emotion'],\n","        num_rows: 838\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset, Audio, Features, Value, ClassLabel, load_metric\n","\n","ds = load_dataset('csv', data_files='/mnt/pci-0000:00:1f.2-ata-1-part1/ZLQ/AI/data/TAL-SER/talser_data.csv',delimiter=';',split='train')\n","ds = ds.cast_column(\"audio\", Audio())\n","ds = ds.train_test_split(test_size=0.2, shuffle=True)\n","ds = ds.remove_columns([\"sex\", \"id\", \"speaker\",'PA'])\n","class_names = [\"积极高唤醒\", \"积极低唤醒\", \"消极高唤醒\", \"消极低唤醒\"]\n","emotion_features = Features({'emotion': ClassLabel(names=class_names)})\n","ds = ds.cast_column(\"emotion\",  ClassLabel(names=class_names))\n","ds=ds.rename_column(\"P\", \"LABEL_0\")\n","ds=ds.rename_column(\"A\", \"LABEL_1\")\n","\n","ds"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-22T13:34:19.641511Z","iopub.status.busy":"2022-05-22T13:34:19.641153Z","iopub.status.idle":"2022-05-22T13:34:28.561245Z","shell.execute_reply":"2022-05-22T13:34:28.560421Z","shell.execute_reply.started":"2022-05-22T13:34:19.641477Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing EmotionModel: ['project_q.weight', 'quantizer.codevectors', 'project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_hid.bias']\n","- This IS expected if you are initializing EmotionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing EmotionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of EmotionModel were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"AttributeError","evalue":"'EmotionModel' object has no attribute 'conv1'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/mnt/pci-0000:00:1f.2-ata-1-part1/ZLQ/AI/SER-transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130362e35352e3139362e38393a363030302f3231392e3234342e3137312e313132227d/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/SER-transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000003vscode-remote?line=67'>68</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model_ft\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130362e35352e3139362e38393a363030302f3231392e3234342e3137312e313132227d/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/SER-transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000003vscode-remote?line=68'>69</a>\u001b[0m \tparam\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130362e35352e3139362e38393a363030302f3231392e3234342e3137312e313132227d/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/SER-transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000003vscode-remote?line=69'>70</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconv1.weights[0, 0, ...]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(model_ft\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39mweight[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130362e35352e3139362e38393a363030302f3231392e3234342e3137312e313132227d/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/SER-transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000003vscode-remote?line=71'>72</a>\u001b[0m \u001b[39m# 3/3 替换fc层(以适应新任务)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130362e35352e3139362e38393a363030302f3231392e3234342e3137312e313132227d/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/SER-transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000003vscode-remote?line=72'>73</a>\u001b[0m num_ftrs \u001b[39m=\u001b[39m model_ft\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39min_features\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'EmotionModel' object has no attribute 'conv1'"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from transformers import Wav2Vec2Processor\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2Model,\n","    Wav2Vec2PreTrainedModel,\n",")\n","\n","\n","class RegressionHead(nn.Module):\n","    r\"\"\"Classification head.\"\"\"\n","\n","    def __init__(self, config):\n","\n","        super().__init__()\n","        print(config.num_labels)\n","        self.config = config\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(config.final_dropout)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, features, **kwargs):\n","\n","        x = features\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","\n","        return x\n","\n","\n","class EmotionModel(Wav2Vec2PreTrainedModel):\n","    r\"\"\"Speech emotion classifier.\"\"\"\n","\n","    def __init__(self, config):\n","\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.config = config\n","        self.wav2vec2 = Wav2Vec2Model(config)\n","        self.classifier = RegressionHead(config)\n","        self.init_weights()\n","\n","    def forward(\n","            self,\n","            input_values,\n","    ):\n","        \n","        outputs = self.wav2vec2(input_values)\n","        hidden_states = outputs[0]\n","        hidden_states = torch.mean(hidden_states, dim=1)\n","        logits = self.classifier(hidden_states)\n","\n","        return hidden_states, logits\n","\n","\n","\n","# load model from hub\n","device = 'cuda'\n","model_name = 'facebook/wav2vec2-base'\n","processor = Wav2Vec2Processor.from_pretrained(model_name)\n","model_ft = EmotionModel.from_pretrained(model_name)\n","\n","# 法1：冻结卷积层\n","for param in model_ft.parameters():\n","\tparam.requires_grad = False\n","print(\"conv1.weights[0, 0, ...]\".format(model_ft.conv1.weight[0, 0, ...]))\n","\n","# 3/3 替换fc层(以适应新任务)\n","num_ftrs = model_ft.fc.in_features\n","model_ft.fc = nn.Linear(in_features=num_ftrs, out_features=2)\n","\n","\n","# dummy signal\n","sampling_rate = 16000\n","signal = np.zeros((1, sampling_rate), dtype=np.float32)\n","\n","\n","# define loss function (criterion), optimizer, and learning rate scheduler\n","criterion = nn.CrossEntropyLoss(ignore_index=-1).cuda(args.gpu)\n","\n","optimizer = torch.optim.SGD(model.parameters(), args.lr,\n","                            momentum=args.momentum,\n","                            weight_decay=args.weight_decay)\n","\n","\"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n","\n","def process_func(\n","    x: np.ndarray,\n","    sampling_rate: int,\n","    embeddings: bool = False,\n",") -> np.ndarray:\n","    r\"\"\"Predict emotions or extract embeddings from raw audio signal.\"\"\"\n","\n","    # run through processor to normalize signal\n","    # always returns a batch, so we just get the first entry\n","    # then we put it on the device\n","    y = processor(x, sampling_rate=sampling_rate)\n","    y = y['input_values'][0]\n","    y = torch.from_numpy(y).to(device)\n","\n","    # run through model\n","    with torch.no_grad():\n","        y = model_ft(y)[0 if embeddings else 1]\n","\n","    # convert to numpy\n","    y = y.detach().cpu().numpy()\n","\n","    return y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/mnt/pci-0000:00:1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m process_func(signal, sampling_rate)\n","\u001b[1;32m/mnt/pci-0000:00:1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb Cell 3'\u001b[0m in \u001b[0;36mprocess_func\u001b[0;34m(x, sampling_rate, embeddings)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=86'>87</a>\u001b[0m \u001b[39m# run through model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=87'>88</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=88'>89</a>\u001b[0m     y \u001b[39m=\u001b[39m model(y)[\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m embeddings \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=90'>91</a>\u001b[0m \u001b[39m# convert to numpy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=91'>92</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32m/mnt/pci-0000:00:1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb Cell 3'\u001b[0m in \u001b[0;36mEmotionModel.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=47'>48</a>\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=48'>49</a>\u001b[0m         input_values,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=49'>50</a>\u001b[0m ):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=51'>52</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwav2vec2(input_values)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=52'>53</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000002vscode-remote?line=53'>54</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(hidden_states, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1347\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1342\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1343\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1344\u001b[0m )\n\u001b[1;32m   1345\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1347\u001b[0m extract_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(input_values)\n\u001b[1;32m   1348\u001b[0m extract_features \u001b[39m=\u001b[39m extract_features\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m   1350\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1351\u001b[0m     \u001b[39m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:515\u001b[0m, in \u001b[0;36mWav2Vec2FeatureEncoder.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    510\u001b[0m         hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    511\u001b[0m             create_custom_forward(conv_layer),\n\u001b[1;32m    512\u001b[0m             hidden_states,\n\u001b[1;32m    513\u001b[0m         )\n\u001b[1;32m    514\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         hidden_states \u001b[39m=\u001b[39m conv_layer(hidden_states)\n\u001b[1;32m    517\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:415\u001b[0m, in \u001b[0;36mWav2Vec2GroupNormConvLayer.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 415\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(hidden_states)\n\u001b[1;32m    416\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m    417\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(hidden_states)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    296\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    297\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    299\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"]}],"source":["process_func(signal, sampling_rate)\n","#  Arousa    dominance valence\n","# [[0.5460759 0.6062269 0.4043165]]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-22T13:34:28.564000Z","iopub.status.busy":"2022-05-22T13:34:28.563063Z","iopub.status.idle":"2022-05-22T13:34:28.567680Z","shell.execute_reply":"2022-05-22T13:34:28.567164Z","shell.execute_reply.started":"2022-05-22T13:34:28.563957Z"},"trusted":true},"outputs":[],"source":["process_func(signal, sampling_rate, embeddings=True)\n","# Pooled hidden states of last transformer layer\n","# [[-0.00752167  0.0065819  -0.00746339 ...  0.00663631  0.00848747\n","#   0.00599209]]"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n"]},{"ename":"ValueError","evalue":"Column to remove ['P', 'A', 'PA'] not in the dataset. Current columns in the dataset: ['LABEL_0', 'LABEL_1', 'audio', 'emotion']","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/mnt/pci-0000:00:1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000044vscode-remote?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m inputs\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000044vscode-remote?line=21'>22</a>\u001b[0m \u001b[39m# 使用 Datasets map函数对整个数据集应用预处理函数。您可以通过设置 batching = True 来加速 map 函数，以便一次处理数据集的多个元素。移除您不需要的列，并重命名 intent_ class 以标记，因为这正是模型所期望的:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000044vscode-remote?line=22'>23</a>\u001b[0m encoded_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mmap(preprocess_function, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mP\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mPA\u001b[39;49m\u001b[39m\"\u001b[39;49m], batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000044vscode-remote?line=23'>24</a>\u001b[0m encoded_dataset \u001b[39m=\u001b[39m encoded_dataset\u001b[39m.\u001b[39mrename_column(\u001b[39m\"\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B219.244.171.112/mnt/pci-0000%3A00%3A1f.2-ata-1-part1/ZLQ/AI/Transformers/522wav2vec2-large-robust-12-ft-emotion-msp-dim.ipynb#ch0000044vscode-remote?line=24'>25</a>\u001b[0m encoded_dataset\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/datasets/dataset_dict.py:770\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    769\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 770\u001b[0m     {\n\u001b[1;32m    771\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    772\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    773\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    774\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    775\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    776\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    777\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    778\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    779\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    780\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    781\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    782\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    783\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    784\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    785\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    786\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    787\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    788\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    789\u001b[0m         )\n\u001b[1;32m    790\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    791\u001b[0m     }\n\u001b[1;32m    792\u001b[0m )\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/datasets/dataset_dict.py:771\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    769\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    770\u001b[0m     {\n\u001b[0;32m--> 771\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    772\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    773\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    774\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    775\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    776\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    777\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    778\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    779\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    780\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    781\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    782\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    783\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    784\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    785\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    786\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    787\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    788\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    789\u001b[0m         )\n\u001b[1;32m    790\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    791\u001b[0m     }\n\u001b[1;32m    792\u001b[0m )\n","File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py:2358\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2355\u001b[0m     remove_columns \u001b[39m=\u001b[39m [remove_columns]\n\u001b[1;32m   2357\u001b[0m \u001b[39mif\u001b[39;00m remove_columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m remove_columns):\n\u001b[0;32m-> 2358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2359\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn to remove \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m col: col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names, remove_columns))\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2360\u001b[0m     )\n\u001b[1;32m   2362\u001b[0m load_from_cache_file \u001b[39m=\u001b[39m load_from_cache_file \u001b[39mif\u001b[39;00m load_from_cache_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_caching_enabled()\n\u001b[1;32m   2364\u001b[0m \u001b[39mif\u001b[39;00m fn_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: Column to remove ['P', 'A', 'PA'] not in the dataset. Current columns in the dataset: ['LABEL_0', 'LABEL_1', 'audio', 'emotion']"]}],"source":["# ## 预处理\n","from transformers import AutoFeatureExtractor\n","\n","feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\",return_attention_mask=True)\n","feature_extractor\n","\n","# preprocess_function\n","# - 调用要加载的音频列，并在必要时重新采样音频文件。\n","# - 检查音频文件的采样率是否与模型预先训练的音频数据的采样率相匹配。您可以在 Wav2Vec2模型卡上找到这些信息。![the Wav2Vec2 model card](https://huggingface.co/docs/transformers/tasks/(https://huggingface.co/facebook/wav2vec2-base).\n","# - 设置一个最大输入长度，这样较长的输入不会被截断\n","\n","metric = load_metric(\"accuracy\")\n","metric\n","max_duration = 10\n","def preprocess_function(examples):\n","    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n","    inputs = processor(\n","        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=int(feature_extractor.sampling_rate * max_duration), truncation=True\n","    )\n","    return inputs\n","    \n","# 使用 Datasets map函数对整个数据集应用预处理函数。您可以通过设置 batching = True 来加速 map 函数，以便一次处理数据集的多个元素。移除您不需要的列，并重命名 intent_ class 以标记，因为这正是模型所期望的:\n","encoded_dataset = ds.map(preprocess_function, remove_columns=[\"audio\",\"emotion\",\"PA\"], batched=True)\n","encoded_dataset\n","\n","## 训练\n","# 使用 AutoModelForAudio 分类加载 wave 2Vec2。指定标签的数量，并将标签数量和标签类之间的映射传递给模型:\n","# from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n","\n","# num_labels = len(id2label)\n","# model = AutoModelForAudioClassification.from_pretrained(\n","#     \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n","    predictions = np.argmax(eval_pred.predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n","\n","\n","# 在 TrainingArguments 中定义训练超参数。\n","training_args = TrainingArguments(\n","    output_dir=\"./wav2vec2-finetuned-ch-emotion-edu\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=3e-5,\n","    num_train_epochs=5,\n","    # prediction_loss_only =False,\n","    warmup_ratio=0.1,\n","    max_grad_norm = 1,# 梯度裁剪功能，控制梯度的最大值，避免过大的梯度给权重带来过大的变化从而使得模型变得不稳定。\n","    lr_scheduler_type = 'linear',\n","    logging_strategy = 'steps',\n","    logging_steps=10,\n","    # load_best_model_at_end=True,\n","    # metric_for_best_model=\"accuracy\",\n","    hub_token = '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',\n","    push_to_hub=True,\n",")\n","\n","# 将训练参数与模型、数据集和特征提取器一起传递给 Trainer。\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_dataset[\"train\"],\n","    eval_dataset=encoded_dataset[\"test\"],\n","    tokenizer=feature_extractor,\n","    compute_metrics=compute_metrics,\n",")\n","trainer.train()\n","trainer.evaluate()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"}}},"nbformat":4,"nbformat_minor":4}
